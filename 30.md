CUDA Tensors
到目前为止，我们一直在CPU内存上分配我们的张量。在进行线性代数运算时，如果你有GPU，那么使用GPU可能是有意义的。要使用GPU，您需要首先在GPU的内存上分配张量。通过名为CUDA的专用API访问GPU。 CUDA API由NVIDIA创建，仅限于在NVIDIA GPU上使用.9 PyTorch提供的CUDA张量对象在使用时与常规CPU绑定器无法区分，除了它们在内部分配的方式。
PyTorch可以很容易地创建这些CUDA张量，将张量从CPU传输到GPU，同时保持其基础类型。 PyTorch中的首选方法是与设备无关，并编写无论是在GPU还是在CPU上运行的代码。在示例116中，我们首先使用torch.cuda.is_available（）检查GPU是否可用，并使用torch.device（）检索设备名称。然后，通过使用.to（设备）方法实例化所有未来的张量并将其移动到目标设备。

例1-16 创建CUDA张量

- input 0
```python
import torch
print (torch.cuda.is_available())
```

- output 0
```
True
```

- input 1
```python
# preferred method: device agnostic tensor instantiation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
print (device)
```

- output 1
```
cuda
```

- input 2
```python
x = torch.rand(3, 3).to(device) 
describe(x)
```

- output 2
```
Type: torch.cuda.FloatTensor 
Shape/size: torch.Size([3, 3]) 
Values:
tensor([[ 0.9149, 0.3993, 0.1100],
[ 0.2541, 0.4333, 0.4451],
[ 0.4966, 0.7865, 0.6604]], device='cuda:0')
```
要对CUDA和nonCUDA对象进行操作，我们需要确保它们位于同一设备上。 如果我们不这样做，计算就会中断，如例子117所示。 例如，当计算不属于计算图的监视度量时，会出现这种情况。 在两个张量对象上操作时，请确保它们都在同一设备上。
