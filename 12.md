预测，也叫估计，是由模型猜到目标的值，考虑到
观察结果。我们这些使用“帽子”符号表示。所以，靶y的预测被表示为
年。
损失函数
损失函数是比较多远关预测的功能是从它的目标
在训练数据的观测。给定一个目标和它的预测，损失函数分配一个标
真正的价值叫损失。损失的值越小，越好的模型在预测
目标。我们用L表示损失函数。
虽然这不是绝对必要的是数学上正式将在NLP生产/深
学模型或写这本书，我们将正式重申监督学习范式
装备读者谁是新的领域与标准术语，让他们有一定的了解
与符号和写作风格的研究论文，他们可能会遇到的arXiv。
考虑一个包含n个示例的数据集。鉴于此数据集，我们要学习的功能
（模型）的F由权重w参数化。也就是说，我们做出关于F的结构假设，以及
考虑到结构，权重的学习值W将充分表征模型。对于给定的
输入X，该模型预测Y作为目标：
在监督学习，培训的例子，我们知道一个观察真正的目标年。损失
此实例然后将L（Y，Y）。然后监督学习成为寻找的过程
最佳参数/权重w，将最小化对所有n个实施例中的累计损失。
培训使用（随机）梯度下降
监督学习的目标是挑选可以最大限度地减少损失函数的参数值
对于给定的数据集。换言之，这等同于在方程式中发现的根源。我们知道
梯度下降是找到一个方程的根的常用技术。回想一下，在传统
梯度下降，我们猜测为根（参数）一些初始值，并更新
参数迭代，直到目标函数（损失函数）的计算结果为低于一个值
可接受的阈值（也称为收敛准则）。对于大型数据集，实现传统
在整个数据集中梯度下降通常是不可能的，由于存储器的限制，而且很
减缓由于计算成本。相反，梯度下降逼近叫
随机梯度下降（SGD）通常采用。在随机的情况下，一个数据点或
数据点的子集，随机挑选，并在梯度计算为该子集。当一个
单个数据点时，该方法被称为纯SGD，而当一个子集（一个以上的）
正在使用的数据点，我们称其为minibatch SGD。通常的话“纯粹”和“minibatch”是
当所使用的方法是明确的基于上下文下降。在实践中，纯粹是新元
很少使用，因为它会导致非常慢的收敛由于嘈杂的更新。有不同
一般SGD算法的变种，都瞄准了更快的收敛。在后面的章节中，我们
探索一些这些变体与梯度如何在更新所述参数用于沿。
迭代更新的参数，这个过程被称为反向传播。每一步（又名
反向传播的历元）由直传和反向通。直传
评估与该参数的当前值的输入，并计算损失函数。该
向后通更新使用损失的梯度的参数。
注意，到现在为止，这里没有什么是特定的深度学习或神经网络。方向
在箭头的
F
igure 11指示数据的“流”而训练系统。我们将有更多的
说的培训和“流”的概念
“
计算图”，但首先，让我们来看看
我们如何能够代表我们的投入和目标，NLP问题的数值，这样我们可以训练
