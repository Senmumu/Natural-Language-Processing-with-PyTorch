#第4章· 自然语言处理的前馈网络

在第3章中，我们通过观察可以存在的最简单的神经网络感知器来介绍神经网络的基础。感知器的历史性垮台之一是它无法学习数据中存在的适度非平凡模式。例如，查看图4-1中绘制的数据点。这相当于任一或（XOR）情况，其中决策边界不能是单条直线（也称为线性可分）。在这种情况下，感知器失败。“Example: Surname Classification with an MLP”.”

Excerpt From: Delip Rao. “Natural Language Processing with PyTorch.” Apple Books. 

图4-1。 XOR数据集中的两个类绘制为圆形和星形。请注意，没有一行可以分隔这两个类。
在本章中，我们将探索一系列传统上称为前馈网络的神经网络模型。我们关注两种前馈神经网络：多层感知器（MLP）和卷积神经网络（CNN）.1多层感知器在结构上扩展了我们在第3章中研究的更简单的感知器，通过将多个感知器分组在一个层中，将多个层堆叠在一起。我们在短时间内介绍了多层感知器，并展示了它们在多类分类中的应用“

摘录自：Delip Rao。 “使用PyTorch进行自然语言处理。”Apple Books。
“本章研究的第二种前馈神经网络，即卷积神经网络，受到数字信号处理中窗口滤波器的深刻启发。通过这种窗口属性，CNN能够在其输入中学习本地化模式，这不仅使它们成为计算机视觉的主力，而且还是检测顺序数据（例如单词和句子）中的子结构的理想候选者。我们在“卷积神经网络”中探索CNN，并在“示例：使用CNN对姓氏进行分类”中演示它们的用法。
在本章中，MLP和CNN组合在一起，因为它们都是前馈神经网络，与不同的神经网络系列（递归神经网络（RNN））形成对比，后者允许反馈（或循环），使得每次计算通过以前的计算得知。在图6和图7中，我们介绍了RNN以及为什么允许网络结构中的循环是有益的。
当我们浏览这些不同的模型时，确保理解工作原理的一种有用方法是在计算数据张量时注意数据张量的大小和形状。每种类型的神经网络层

“多层感知器被认为是最基本的神经网络构建模块之一。最简单的MLP是第3章感知器的扩展。感知器将数据向量2作为输入并计算单个输出值。在MLP中，许多感知器被分组，使得单个层的输出是新的矢量而不是单个输出值。在PyTorch中，您将在后面看到，只需在线性图层中设置输出要素的数量即可完成。 MLP的另一个方面是它将多个层与每层之间的非线性组合在一起。
最简单的MLP，如图4-2所示，由三个表示阶段和两个线性层组成。第一阶段是输入向量。这是给模型的矢量。在“示例：对餐馆评论的情感进行分类”中，输入向量是Yelp评论的折叠单热表示。给定输入向量，第一个线性层计算隐藏向量 - 第二阶段“

“一个简单的例子：异或
让我们看看前面描述的XOR示例，看看感知器与MLP会发生什么。在这个例子中，我们在二元分类任务中训练感知器和MLP：识别星星和圆圈。每个数据点都是2D坐标。如果不深入了解实现细节，最终的模型预测如图4-3所示。在此图中，错误分类的数据点用黑色填充，而正确分类的数据点未填充。在左侧面板中，您可以看到感知器难以学习可以分离星形和圆形的决策边界，由填充的形状证明。然而，MLP（右图）学习了一个决策边界，可以更准确地对星星和圆圈进行分类。

图4-3。来自感知器（左）和MLP（右）的学习解决方案用于XOR问题。每个数据点的真实类别是点的形状：星形或圆形。不正确的分类用黑色填充，并且没有填写正确的分类。这些行是每个模型的决策边界。在左侧面板中，感知器